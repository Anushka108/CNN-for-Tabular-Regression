{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8797164,
          "sourceType": "datasetVersion",
          "datasetId": 5289783
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "CNN_for_Tabular_Regression",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anushka108/CNN-for-Tabular-Regression/blob/main/CNN_for_Tabular_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'restaurant-revenue-prediction-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5289783%2F8797164%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240707%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240707T050933Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D20b20d7c0051b9e0312553ea6486e25673baed975fb8f341b863bfb2a5104c98c6950603e7310034a13c3c7f9a7ce2fe9e0bd4217bae7957f9fd939b47adb49e3f4463cb6b0771b7eb2b0109022c16ca349d9c7fc4415f0b243ea2976a7751b37c5c4a0285c7e22df23c75c2cf64636ccf844fdd26bb1bfb482a315d7aab6d5b41f31514d923fc2d566e5e9c80488feb7ac34464551da670898b7113c2cf1d9d6ddb1f147aa58f9e2352079935e35998c424b4a4730cfb9634320a941cb13eb5bbf68ad0949109588a9bed93986d7dcefb64bf469e28a6174df22cf3e8abf3ffaea8a0b6efc9e2fa9a240f5f707708dfb3d4a0ac6c27f7d66450251692c26559'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "JIpoymM9mQGE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing Libraries**"
      ],
      "metadata": {
        "id": "b9amlP23mQGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # pandas for data manipulation\n",
        "import matplotlib.pyplot as plt # Matplot.lib for data visualisations\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler # Normalisation model\n",
        "from sklearn.model_selection import train_test_split # Splitting data to train and test proportions\n",
        "from sklearn.metrics import mean_squared_error, r2_score # Evaluation metrics\n",
        "from tensorflow.keras.models import Sequential # importing the base model\n",
        "from tensorflow.keras.layers import Dense, Dropout # importing hidden layers\n",
        "from tensorflow.keras.optimizers import Adam # importing optimiser\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # Callbacck functions to confirm  betterment in model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-03T04:36:00.832132Z",
          "iopub.execute_input": "2024-07-03T04:36:00.832838Z",
          "iopub.status.idle": "2024-07-03T04:36:00.840354Z",
          "shell.execute_reply.started": "2024-07-03T04:36:00.832806Z",
          "shell.execute_reply": "2024-07-03T04:36:00.839179Z"
        },
        "trusted": true,
        "id": "Mg60QHlcmQGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data loading**"
      ],
      "metadata": {
        "id": "OhNH4BEvmQGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "df = pd.read_csv(\"/kaggle/input/restaurant-revenue-prediction-dataset/restaurant_data.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-03T04:36:00.842488Z",
          "iopub.execute_input": "2024-07-03T04:36:00.842837Z",
          "iopub.status.idle": "2024-07-03T04:36:00.915401Z",
          "shell.execute_reply.started": "2024-07-03T04:36:00.842809Z",
          "shell.execute_reply": "2024-07-03T04:36:00.91435Z"
        },
        "trusted": true,
        "id": "G7E70OOFmQGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing data**"
      ],
      "metadata": {
        "id": "DXalHHxxmQGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-03T04:36:00.916725Z",
          "iopub.execute_input": "2024-07-03T04:36:00.917058Z",
          "iopub.status.idle": "2024-07-03T04:36:00.930291Z",
          "shell.execute_reply.started": "2024-07-03T04:36:00.917031Z",
          "shell.execute_reply": "2024-07-03T04:36:00.929218Z"
        },
        "trusted": true,
        "id": "BJ13y_Z6mQGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-03T04:36:00.93241Z",
          "iopub.execute_input": "2024-07-03T04:36:00.933166Z",
          "iopub.status.idle": "2024-07-03T04:36:00.967569Z",
          "shell.execute_reply.started": "2024-07-03T04:36:00.933135Z",
          "shell.execute_reply": "2024-07-03T04:36:00.966537Z"
        },
        "trusted": true,
        "id": "Jmb3ZZrgmQGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "trusted": true,
        "id": "8I8g89i0mQGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding categorical variable**"
      ],
      "metadata": {
        "id": "yeT7mGBqmQGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining Label encoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# implementing the loop to encode every columns with object\n",
        "\n",
        "for column in df.select_dtypes(include=['object']).columns:\n",
        "    df[column] = label_encoder.fit_transform(df[column])"
      ],
      "metadata": {
        "trusted": true,
        "id": "LDxG0IP4mQGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting features and target\n",
        "\n",
        "X = df.drop('Revenue', axis=1)\n",
        "y = df['Revenue']"
      ],
      "metadata": {
        "trusted": true,
        "id": "mqDjNBtimQGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test portions\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "trusted": true,
        "id": "5psgw2kEmQGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalising data**"
      ],
      "metadata": {
        "id": "l85_qDoamQGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalising data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "_B8HWcjamQGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model setting up**"
      ],
      "metadata": {
        "id": "-438DpUZmQGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],), kernel_initializer='he_normal'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu', kernel_initializer='he_normal'),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "trusted": true,
        "id": "ss1F2t4nmQGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')"
      ],
      "metadata": {
        "trusted": true,
        "id": "-1-0AMSNmQGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining callbacks\n",
        "early_stopping = EarlyStopping(patience=20, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(factor=0.2, patience=10, min_lr=1e-6)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Wjl9ryOlmQGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding intermediate visualization\n",
        "epochs = 500 # number of times the model eill train on data\n",
        "batch_size = 32 # number of samples a model trains on in 1 iterations\n",
        "validation_split = 0.2 # Validation split for evaluation\n",
        "interval = 100  # Interval for intermediate visualization"
      ],
      "metadata": {
        "trusted": true,
        "id": "oJfRaENamQGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_scaled.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2xCq-6JEmQGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initialising training**"
      ],
      "metadata": {
        "id": "ZGksKIGimQGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=validation_split,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "8yabURdMmQGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plotting mid training curves**"
      ],
      "metadata": {
        "id": "P4FtlLRsmQGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intermediate visualization function\n",
        "def plot_intermediate(history, interval, epochs):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    for i in range(0, epochs, interval):\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['loss'][:i+1], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'][:i+1], label='Validation Loss')\n",
        "        plt.title(f'Model Loss at Epoch {i+1}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "plot_intermediate(history, interval, epochs)"
      ],
      "metadata": {
        "trusted": true,
        "id": "IBmS0rDpmQGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Evaluation**"
      ],
      "metadata": {
        "id": "EXOdZuynmQGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared Score: {r2}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "fgr6ocJGmQGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting final training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "8Krpl0eMmQGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}